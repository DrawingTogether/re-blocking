{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Vf0QUYZJ0AYf"},"outputs":[],"source":["import cv2\n","import numpy as np\n","from skimage.metrics import structural_similarity as ssim # install scikit-image\n","from sklearn.cluster import KMeans # install scikit-learn"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# read images and convert to RGB\n","## image 1 = generated image\n","## image 2 = real image\n","\n","image1 = cv2.imread(\"ny-m-epoch207_fake_B.png\")\n","image1 = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n","\n","image2 = cv2.imread(\"ny-m-epoch207_real_B.png\")\n","image2 = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# pixel count for specific color range\n","\n","# Define the color range\n","lower_red = np.array([0, 0, 100])\n","upper_red = np.array([50, 56, 255])\n","\n","# Create a mask that only includes the defined color range\n","mask = cv2.inRange(image1, lower_red, upper_red)\n","\n","# Count the pixels\n","red_pixel_count = cv2.countNonZero(mask)\n","\n","print(f'The number of red pixels is: {red_pixel_count}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# pixel count for all colors\n","\n","# Flatten the image array to 2D, each row is a pixel, with columns for B, G, and R values\n","pixels = image1.reshape(-1, 3)\n","\n","# Convert pixels to a more manageable type\n","pixels = np.array(pixels, dtype=np.uint8)\n","\n","# Find unique colors and their counts\n","unique_colors, counts = np.unique(pixels, axis=0, return_counts=True)\n","\n","# Print or process the unique colors and their counts\n","for color, count in zip(unique_colors, counts):\n","    print(f\"Color: {color}, Count: {count}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# quantize the colors\n","\n","def quantize_image(image, n_colors=8):\n","    # Reshape the image to a 2D array of pixels\n","    pixels = image.reshape(-1, 3)\n","    \n","    # Use KMeans clustering to find the most dominant colors\n","    kmeans = KMeans(n_clusters=n_colors, random_state=0).fit(pixels)\n","    \n","    # Map each pixel to the nearest color centroid\n","    new_pixels = kmeans.cluster_centers_[kmeans.labels_]\n","    \n","    # Reshape back to the original image shape\n","    quantized_image = new_pixels.reshape(image.shape).astype('uint8')\n","    \n","    return quantized_image, kmeans.cluster_centers_"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# count the pixels of each quantized color\n","\n","def count_pixels(image, centroids):\n","    unique, counts = np.unique(image.reshape(-1, image.shape[2]), axis=0, return_counts=True)\n","    color_counts = {tuple(centroids[i]): count for i, count in enumerate(counts)}\n","    return color_counts"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Quantize the images\n","quantized_image1, centroids1 = quantize_image(image1, n_colors=8)\n","quantized_image2, centroids2 = quantize_image(image2, n_colors=8)\n","\n","# Count the pixels for each quantized color\n","color_counts1 = count_pixels(quantized_image1, centroids1)\n","color_counts2 = count_pixels(quantized_image2, centroids2)\n","\n","# Compare the pixel counts\n","# This is a simple comparison; for more detailed analysis, you may need to align the centroids\n","# and compare the counts for each matched color.\n","print(\"Image 1 Color Counts:\")\n","for color, count in color_counts1.items():\n","    print(f\"Color: {color}, Count: {count}\")\n","\n","print(\"\\nImage 2 Color Counts:\")\n","for color, count in color_counts2.items():\n","    print(f\"Color: {color}, Count: {count}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Mean Squared Error (MSE)\n","## MSE calculates the average squared difference between the pixel intensities of two images. \n","## It gives a single number that represents the error between the two images; the lower the MSE, the more similar the images are.\n","## MSE is very sensitive to even slight changes in pixel values.\n","\n","def mse(imageA, imageB):\n","    # The MSE between the two images is the sum of the squared difference divided by the number of pixels\n","    err = np.sum((imageA.astype(\"float\") - imageB.astype(\"float\")) ** 2)\n","    err /= float(imageA.shape[0] * imageA.shape[1])\n","    \n","    return err\n","\n","# Ensure the images are the same size\n","image1 = cv2.resize(image1, (image2.shape[1], image2.shape[0]))\n","\n","# Compare the images\n","print(f\"MSE: {mse(image1, image2)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Structural Similarity Index (SSIM)\n","## SSIM measures the similarity between two images in terms of luminance, contrast, and structure. \n","## other than MSE, SSIM can be more aligned with human visual perception. \n","## The SSIM index is a decimal value between -1 and 1; a value of 1 indicates perfect similarity.\n","\n","def compare_images(imageA, imageB):\n","    # Compute SSIM between two images\n","    s = ssim(imageA, imageB, multichannel=True)\n","    \n","    return s\n","\n","# Convert the images to grayscale\n","image1_gray = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n","image2_gray = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n","\n","# Ensure the images are the same size\n","image1_gray = cv2.resize(image1_gray, (image2_gray.shape[1], image2_gray.shape[0]))\n","\n","# Compare the images\n","print(f\"SSIM: {compare_images(image1_gray, image2_gray)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Feature matching \n","## Feature matching techniques, such as SIFT (Scale-Invariant Feature Transform), SURF (Speeded-Up Robust Features), or ORB (Oriented FAST and Rotated BRIEF), detect and describe local features in images. \n","## These features can then be matched across images to find similarities, even in the presence of scale, rotation, and illumination changes.\n","\n","# Initialize ORB detector\n","orb = cv2.ORB_create()\n","\n","# Find the keypoints and descriptors with ORB\n","kp1, des1 = orb.detectAndCompute(image1, None)\n","kp2, des2 = orb.detectAndCompute(image2, None)\n","\n","# Create BFMatcher object\n","bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n","\n","# Match descriptors\n","matches = bf.match(des1, des2)\n","\n","# Sort them in the order of their distance\n","matches = sorted(matches, key=lambda x:x.distance)\n","\n","# Draw first 10 matches\n","result = cv2.drawMatches(image1, kp1, image2, kp2, matches[:10], None, flags=2)\n","\n","# Display the result\n","cv2.imshow(\"Matches\", result)\n","cv2.waitKey(0)\n","cv2.destroyAllWindows()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Initialize SIFT detector\n","## look into tolerance\n","sift = cv2.SIFT_create()\n","\n","# Detect and compute the descriptors with SIFT\n","keypoints1, descriptors1 = sift.detectAndCompute(image1, None)\n","keypoints2, descriptors2 = sift.detectAndCompute(image2, None)\n","\n","# Create BFMatcher object\n","bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n","\n","# Match descriptors\n","matches = bf.match(descriptors1, descriptors2)\n","\n","# Sort them in the order of their distance\n","matches = sorted(matches, key=lambda x:x.distance)\n","\n","# Draw first 10 matches\n","img_matches = cv2.drawMatches(image1, keypoints1, image2, keypoints2, matches[:10], None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n","\n","# Display the matches\n","cv2.imshow(\"SIFT Matches\", img_matches)\n","cv2.waitKey(0)\n","cv2.destroyAllWindows()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Initialize SURF detector\n","surf = cv2.xfeatures2d.SURF_create(400)  # Threshold for hessian keypoint detector\n","\n","# Detect and compute the descriptors with SURF\n","keypoints1, descriptors1 = surf.detectAndCompute(image1, None)\n","keypoints2, descriptors2 = surf.detectAndCompute(image2, None)\n","\n","# Create BFMatcher object\n","bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n","\n","# Match descriptors\n","matches = bf.match(descriptors1, descriptors2)\n","\n","# Sort them in the order of their distance\n","matches = sorted(matches, key=lambda x:x.distance)\n","\n","# Draw first 10 matches\n","img_matches = cv2.drawMatches(image1, keypoints1, image2, keypoints2, matches[:10], None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n","\n","# Display the matches\n","cv2.imshow(\"SURF Matches\", img_matches)\n","cv2.waitKey(0)\n","cv2.destroyAllWindows()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Union over Intersection (IoU)\n","## IoU is a measure of the overlap between two images.\n","\n","def quantize_image(image, n_colors=8):\n","    # Reshape the image to a 2D array of pixels\n","    pixels = image.reshape(-1, 3)\n","    \n","    # Use KMeans clustering to find the most dominant colors\n","    kmeans = KMeans(n_clusters=n_colors, random_state=0).fit(pixels)\n","    \n","    # Map each pixel to the nearest color centroid\n","    new_pixels = kmeans.cluster_centers_[kmeans.labels_]\n","    \n","    # Reshape back to the original image shape\n","    quantized_image = new_pixels.reshape(image.shape).astype('uint8')\n","    \n","    return quantized_image, kmeans.cluster_centers_\n","\n","def count_pixels(image):\n","    # Flatten the image and convert to a tuple to make it hashable\n","    pixels = [tuple(pixel) for pixel in image.reshape(-1, 3)]\n","    # Count each unique color\n","    color_counts = {color: pixels.count(color) for color in set(pixels)}\n","    return color_counts\n","\n","def calculate_uoi(color_counts1, color_counts2):\n","    colors1 = set(color_counts1.keys())\n","    colors2 = set(color_counts2.keys())\n","    \n","    intersection = colors1.intersection(colors2)\n","    union = colors1.union(colors2)\n","    \n","    uoi = len(intersection) / len(union) if len(union) > 0 else 0\n","    return uoi\n","\n","# Quantize the images\n","quantized_image1, _ = quantize_image(image1, n_colors=8)\n","quantized_image2, _ = quantize_image(image2, n_colors=8)\n","\n","# Count the pixels for each quantized color\n","color_counts1 = count_pixels(quantized_image1)\n","color_counts2 = count_pixels(quantized_image2)\n","\n","# Calculate UoI\n","uoi = calculate_uoi(color_counts1, color_counts2)\n","print(f\"Union over Intersection (UoI) for color comparison: {uoi}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## create and use mask for training dataset (black and white)\n","## single color for all parcels and a white background (for parcel dataset only)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":0}
