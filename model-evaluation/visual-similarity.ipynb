{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"Vf0QUYZJ0AYf"},"outputs":[],"source":["import cv2\n","import numpy as np\n","from skimage.metrics import structural_similarity as ssim # install scikit-image\n","from sklearn.cluster import KMeans # install scikit-learn"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# read images and convert to RGB\n","## image 1 = generated image\n","## image 2 = real image\n","\n","image1 = cv2.imread(\"ny-m-epoch207_fake_B.png\")\n","image1 = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n","\n","image2 = cv2.imread(\"ny-m-epoch207_real_B.png\")\n","image2 = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The number of red pixels is: 1761\n"]}],"source":["# pixel count for specific color range\n","\n","# Define the color range\n","lower_red = np.array([0, 0, 100])\n","upper_red = np.array([50, 56, 255])\n","\n","# Create a mask that only includes the defined color range\n","mask = cv2.inRange(image1, lower_red, upper_red)\n","\n","# Count the pixels\n","red_pixel_count = cv2.countNonZero(mask)\n","\n","print(f'The number of red pixels is: {red_pixel_count}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# pixel count for all colors\n","\n","# Flatten the image array to 2D, each row is a pixel, with columns for B, G, and R values\n","pixels = image1.reshape(-1, 3)\n","\n","# Convert pixels to a more manageable type\n","pixels = np.array(pixels, dtype=np.uint8)\n","\n","# Find unique colors and their counts\n","unique_colors, counts = np.unique(pixels, axis=0, return_counts=True)\n","\n","# Print or process the unique colors and their counts\n","for color, count in zip(unique_colors, counts):\n","    print(f\"Color: {color}, Count: {count}\")"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# quantize the colors\n","\n","def quantize_image(image, n_colors=8):\n","    # Reshape the image to a 2D array of pixels\n","    pixels = image.reshape(-1, 3)\n","    \n","    # Use KMeans clustering to find the most dominant colors\n","    kmeans = KMeans(n_clusters=n_colors, random_state=0).fit(pixels)\n","    \n","    # Map each pixel to the nearest color centroid\n","    new_pixels = kmeans.cluster_centers_[kmeans.labels_]\n","    \n","    # Reshape back to the original image shape\n","    quantized_image = new_pixels.reshape(image.shape).astype('uint8')\n","    \n","    return quantized_image, kmeans.cluster_centers_"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# count the pixels of each quantized color\n","\n","def count_pixels(image, centroids):\n","    unique, counts = np.unique(image.reshape(-1, image.shape[2]), axis=0, return_counts=True)\n","    color_counts = {tuple(centroids[i]): count for i, count in enumerate(counts)}\n","    return color_counts"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Image 1 Color Counts:\n","Color: (187.35173259736277, 181.1344168455484, 169.94940202391905), Count: 3230\n","Color: (51.44754503708937, 59.949699752737544, 50.81455316142706), Count: 6830\n","Color: (182.29723594605423, 65.09213513152623, 211.03418346908802), Count: 14153\n","Color: (45.871779859484775, 189.84426229508196, 141.90427400468383), Count: 13847\n","Color: (186.64299802761343, 189.65253122945433, 46.24161735700197), Count: 4106\n","Color: (104.39621687964768, 120.03631506750415, 98.33362212114649), Count: 7488\n","Color: (181.7810218978102, 52.87104622871046, 87.8588807785888), Count: 6103\n","Color: (42.74381188118812, 47.35179455445544, 171.68316831683168), Count: 9779\n","\n","Image 2 Color Counts:\n","Color: (187.9523478606175, 180.44197359277274, 170.42082795592177), Count: 2889\n","Color: (105.18124507486209, 115.55785395324402, 94.28230890464933), Count: 12647\n","Color: (183.67885714285717, 63.15428571428571, 214.19285714285715), Count: 7924\n","Color: (187.4431818181818, 54.07843680709534, 91.68486696230599), Count: 15226\n","Color: (45.478006329113924, 58.35799050632911, 49.83488924050633), Count: 6994\n","Color: (40.79085872576178, 31.252077562326875, 167.72506925207756), Count: 3608\n","Color: (46.409997475385, 184.39207270891188, 150.15147689977277), Count: 10071\n","Color: (188.96146996924074, 192.36716852841187, 44.38092925368302), Count: 6177\n"]}],"source":["# Quantize the images\n","quantized_image1, centroids1 = quantize_image(image1, n_colors=8)\n","quantized_image2, centroids2 = quantize_image(image2, n_colors=8)\n","\n","# Count the pixels for each quantized color\n","color_counts1 = count_pixels(quantized_image1, centroids1)\n","color_counts2 = count_pixels(quantized_image2, centroids2)\n","\n","# Compare the pixel counts\n","# This is a simple comparison; for more detailed analysis, you may need to align the centroids\n","# and compare the counts for each matched color.\n","print(\"Image 1 Color Counts:\")\n","for color, count in color_counts1.items():\n","    print(f\"Color: {color}, Count: {count}\")\n","\n","print(\"\\nImage 2 Color Counts:\")\n","for color, count in color_counts2.items():\n","    print(f\"Color: {color}, Count: {count}\")"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["MSE: 2034.570556640625\n"]}],"source":["# Mean Squared Error (MSE)\n","## MSE calculates the average squared difference between the pixel intensities of two images. \n","## It gives a single number that represents the error between the two images; the lower the MSE, the more similar the images are.\n","## MSE is very sensitive to even slight changes in pixel values.\n","\n","def mse(imageA, imageB):\n","    # The MSE between the two images is the sum of the squared difference divided by the number of pixels\n","    err = np.sum((imageA.astype(\"float\") - imageB.astype(\"float\")) ** 2)\n","    err /= float(imageA.shape[0] * imageA.shape[1])\n","    \n","    return err\n","\n","# Ensure the images are the same size\n","image1 = cv2.resize(image1, (image2.shape[1], image2.shape[0]))\n","\n","# Compare the images\n","print(f\"MSE: {mse(image1, image2)}\")"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["SSIM: 0.8097080619926511\n"]}],"source":["# Structural Similarity Index (SSIM)\n","## SSIM measures the similarity between two images in terms of luminance, contrast, and structure. \n","## ther than MSE, SSIM can be more aligned with human visual perception. \n","## The SSIM index is a decimal value between -1 and 1; a value of 1 indicates perfect similarity.\n","\n","def compare_images(imageA, imageB):\n","    # Compute SSIM between two images\n","    s = ssim(imageA, imageB, multichannel=True)\n","    \n","    return s\n","\n","# Convert the images to grayscale\n","image1_gray = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n","image2_gray = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n","\n","# Ensure the images are the same size\n","image1_gray = cv2.resize(image1_gray, (image2_gray.shape[1], image2_gray.shape[0]))\n","\n","# Compare the images\n","print(f\"SSIM: {compare_images(image1_gray, image2_gray)}\")"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# Feature matching \n","## Feature matching techniques, such as SIFT (Scale-Invariant Feature Transform), SURF (Speeded-Up Robust Features), or ORB (Oriented FAST and Rotated BRIEF), detect and describe local features in images. \n","## These features can then be matched across images to find similarities, even in the presence of scale, rotation, and illumination changes.\n","\n","# Initialize ORB detector\n","orb = cv2.ORB_create()\n","\n","# Find the keypoints and descriptors with ORB\n","kp1, des1 = orb.detectAndCompute(image1, None)\n","kp2, des2 = orb.detectAndCompute(image2, None)\n","\n","# Create BFMatcher object\n","bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n","\n","# Match descriptors\n","matches = bf.match(des1, des2)\n","\n","# Sort them in the order of their distance\n","matches = sorted(matches, key=lambda x:x.distance)\n","\n","# Draw first 10 matches\n","result = cv2.drawMatches(image1, kp1, image2, kp2, matches[:10], None, flags=2)\n","\n","# Display the result\n","cv2.imshow(\"Matches\", result)\n","cv2.waitKey(0)\n","cv2.destroyAllWindows()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":0}
