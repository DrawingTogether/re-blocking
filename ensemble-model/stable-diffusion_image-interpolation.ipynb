{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check MPS availability on Apple Silicon\n",
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"MPS backend is available.\")\n",
    "else:\n",
    "    print(\"MPS backend is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1_path = \"/Users/ls/Library/CloudStorage/GoogleDrive-l.schrage@northeastern.edu/Shared drives/Drawing Participation/Million Neighborhoods/Generated Images/ma-boston/parcels/parcels_50.jpg\" \n",
    "image2_path = \"/Users/ls/Library/CloudStorage/GoogleDrive-l.schrage@northeastern.edu/Shared drives/Drawing Participation/Million Neighborhoods/Generated Images/pa-pittsburgh/parcels/parcels_263.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to MPS if available\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the pre-trained Stable Diffusion model\n",
    "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id)\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "# Enable attention slicing to reduce memory usage\n",
    "pipe.enable_attention_slicing()\n",
    "\n",
    "# Define transformations for input images\n",
    "transform = Compose([\n",
    "    Resize((256, 256)),  # Further reduce input size to save memory\n",
    "    ToTensor(),\n",
    "    Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "# Function to load and transform images\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    return transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "# Load images\n",
    "print(\"Loading and transforming images...\")\n",
    "image1 = load_image(image1_path).float()\n",
    "image2 = load_image(image2_path).float()\n",
    "\n",
    "# Clear MPS cache to free up memory\n",
    "torch.mps.empty_cache()\n",
    "\n",
    "# Encode images to latents\n",
    "print(\"Encoding images to latent representations...\")\n",
    "with torch.no_grad():\n",
    "    latents1 = pipe.vae.encode(image1).latent_dist.sample()\n",
    "    latents2 = pipe.vae.encode(image2).latent_dist.sample()\n",
    "\n",
    "# Interpolate between latents\n",
    "print(\"Interpolating between latents...\")\n",
    "num_steps = 10\n",
    "alphas = np.linspace(0, 1, num_steps)\n",
    "interpolated_images = []\n",
    "\n",
    "for idx, alpha in enumerate(alphas):\n",
    "    print(f\"Interpolating step {idx + 1}/{num_steps}...\")\n",
    "    interpolated_latent = (1 - alpha) * latents1 + alpha * latents2\n",
    "    decoded_image = pipe.vae.decode(interpolated_latent).sample[0]\n",
    "    interpolated_images.append(decoded_image)\n",
    "\n",
    "# Plot results\n",
    "print(\"Plotting results...\")\n",
    "fig, axes = plt.subplots(1, num_steps, figsize=(20, 5))\n",
    "for ax, img in zip(axes, interpolated_images):\n",
    "    ax.imshow((img.permute(1, 2, 0).cpu().numpy() * 0.5 + 0.5))\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
