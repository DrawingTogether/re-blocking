{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Data generation and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from tqdm.notebook import tqdm\n",
    "import signal\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, original_images, target_images, output_images, labels, transform=None):\n",
    "        self.original_images = original_images\n",
    "        self.target_images = target_images\n",
    "        self.output_images = output_images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        original_image = self.original_images[idx]\n",
    "        target_image = self.target_images[idx]\n",
    "        output_image = self.output_images[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            original_image = self.transform(original_image)\n",
    "            target_image = self.transform(target_image)\n",
    "            output_image = self.transform(output_image)\n",
    "        return original_image, target_image, output_image, label\n",
    "\n",
    "# Define a timeout handler\n",
    "def handler(signum, frame):\n",
    "    raise Exception(\"Image loading timed out\")\n",
    "\n",
    "# Set the signal handler and a 5-second alarm\n",
    "signal.signal(signal.SIGALRM, handler)\n",
    "\n",
    "# Function to load a single image\n",
    "def load_single_image(img_path, transform):\n",
    "    try:\n",
    "        signal.alarm(5)  # Trigger alarm in 5 seconds\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img = transform(img)\n",
    "        signal.alarm(0)  # Disable the alarm\n",
    "        return img\n",
    "    except (UnidentifiedImageError, Exception) as e:\n",
    "        print(f\"Cannot process image file {img_path}, skipping. Error: {e}\")\n",
    "        signal.alarm(0)  # Disable the alarm in case of an exception\n",
    "        return None\n",
    "\n",
    "# Function to load images from a directory, filtering by a keyword, with parallel processing\n",
    "def load_images_from_directory(directory, keyword=None, target_size=(256, 256), max_workers=8):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(target_size),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    images = []\n",
    "    filenames = [f for f in sorted(os.listdir(directory)) if keyword is None or keyword in f]\n",
    "    img_paths = [os.path.join(directory, f) for f in filenames]\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        for img in tqdm(executor.map(lambda p: load_single_image(p, transform), img_paths), total=len(img_paths), desc=f\"Loading images from {directory}\"):\n",
    "            if img is not None:\n",
    "                images.append(img)\n",
    "    \n",
    "    return torch.stack(images)\n",
    "\n",
    "# Directories\n",
    "input_dir1_original = '/Users/ls/Library/CloudStorage/GoogleDrive-l.schrage@northeastern.edu/Shared drives/Drawing Participation/Million Neighborhoods/Generated Images/ma-boston/buildings'\n",
    "input_dir1_target = '/Users/ls/Library/CloudStorage/GoogleDrive-l.schrage@northeastern.edu/Shared drives/Drawing Participation/Million Neighborhoods/Generated Images/ma-boston/parcels'\n",
    "output_dir1 = '/Users/ls/Library/CloudStorage/GoogleDrive-l.schrage@northeastern.edu/Shared drives/Drawing Participation/Million Neighborhoods/Trained Models/ma-boston-p2p-500-150-v100/web-boston/images'\n",
    "input_dir2_original = '/Users/ls/Library/CloudStorage/GoogleDrive-l.schrage@northeastern.edu/Shared drives/Drawing Participation/Million Neighborhoods/Generated Images/nc-charlotte/buildings'\n",
    "input_dir2_target = '/Users/ls/Library/CloudStorage/GoogleDrive-l.schrage@northeastern.edu/Shared drives/Drawing Participation/Million Neighborhoods/Generated Images/nc-charlotte/parcels'\n",
    "output_dir2 = '/Users/ls/Library/CloudStorage/GoogleDrive-l.schrage@northeastern.edu/Shared drives/Drawing Participation/Million Neighborhoods/Trained Models/nc-charlotte-500-150-v100/web-charlotte/images'\n",
    "input_dir3_original = '/Users/ls/Library/CloudStorage/GoogleDrive-l.schrage@northeastern.edu/Shared drives/Drawing Participation/Million Neighborhoods/Generated Images/ny-manhattan/buildings'\n",
    "input_dir3_target = '/Users/ls/Library/CloudStorage/GoogleDrive-l.schrage@northeastern.edu/Shared drives/Drawing Participation/Million Neighborhoods/Generated Images/ny-manhattan/parcels'\n",
    "output_dir3 = '/Users/ls/Library/CloudStorage/GoogleDrive-l.schrage@northeastern.edu/Shared drives/Drawing Participation/Million Neighborhoods/Trained Models/ny-manhattan-p2p-500-150-v100/web-manhattan/images'\n",
    "input_dir4_original = '/Users/ls/Library/CloudStorage/GoogleDrive-l.schrage@northeastern.edu/Shared drives/Drawing Participation/Million Neighborhoods/Generated Images/pa-pittsburgh/buildings'\n",
    "input_dir4_target = '/Users/ls/Library/CloudStorage/GoogleDrive-l.schrage@northeastern.edu/Shared drives/Drawing Participation/Million Neighborhoods/Generated Images/pa-pittsburgh/parcels'\n",
    "output_dir4 = '/Users/ls/Library/CloudStorage/GoogleDrive-l.schrage@northeastern.edu/Shared drives/Drawing Participation/Million Neighborhoods/Trained Models/pa-pittsburgh-p2p-500-150-v100/web-pittsburgh/images'\n",
    "\n",
    "# Output save directory\n",
    "save_dir = '/Users/ls/Library/CloudStorage/GoogleDrive-l.schrage@northeastern.edu/Shared drives/Drawing Participation/Million Neighborhoods/Ensemble Model/Step 1 outputs'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Load original and target input images with parallel processing\n",
    "input_images_model1_original = load_images_from_directory(input_dir1_original, '', max_workers=8)\n",
    "input_images_model1_target = load_images_from_directory(input_dir1_target, '', max_workers=8)\n",
    "input_images_model2_original = load_images_from_directory(input_dir2_original, '', max_workers=8)\n",
    "input_images_model2_target = load_images_from_directory(input_dir2_target, '', max_workers=8)\n",
    "input_images_model3_original = load_images_from_directory(input_dir3_original, '', max_workers=8)\n",
    "input_images_model3_target = load_images_from_directory(input_dir3_target, '', max_workers=8)\n",
    "input_images_model4_original = load_images_from_directory(input_dir4_original, '', max_workers=8)\n",
    "input_images_model4_target = load_images_from_directory(input_dir4_target, '', max_workers=8)\n",
    "\n",
    "# Load only the fake_B output images with parallel processing\n",
    "output_images_model1 = load_images_from_directory(output_dir1, 'fake_B', max_workers=8)\n",
    "output_images_model2 = load_images_from_directory(output_dir2, 'fake_B', max_workers=8)\n",
    "output_images_model3 = load_images_from_directory(output_dir3, 'fake_B', max_workers=8)\n",
    "output_images_model4 = load_images_from_directory(output_dir4, 'fake_B', max_workers=8)\n",
    "\n",
    "# Create dataset and labels\n",
    "original_images = []\n",
    "target_images = []\n",
    "output_images_all = []\n",
    "labels = []\n",
    "\n",
    "# Using a progress bar for the image and label appending loop\n",
    "for i in tqdm(range(len(input_images_model1_original)), desc=\"Combining images and labels\"):\n",
    "    original_images.append(input_images_model1_original[i])\n",
    "    target_images.append(input_images_model1_target[i])\n",
    "    output_images_all.append(output_images_model1[i])\n",
    "    labels.append(0)  # Label for model 1\n",
    "    original_images.append(input_images_model2_original[i])\n",
    "    target_images.append(input_images_model2_target[i])\n",
    "    output_images_all.append(output_images_model2[i])\n",
    "    labels.append(1)  # Label for model 2\n",
    "    original_images.append(input_images_model3_original[i])\n",
    "    target_images.append(input_images_model3_target[i])\n",
    "    output_images_all.append(output_images_model3[i])\n",
    "    labels.append(2)  # Label for model 3\n",
    "    original_images.append(input_images_model4_original[i])\n",
    "    target_images.append(input_images_model4_target[i])\n",
    "    output_images_all.append(output_images_model4[i])\n",
    "    labels.append(3)  # Label for model 4\n",
    "\n",
    "# Convert dataset and labels to tensors\n",
    "original_images = torch.stack(original_images)\n",
    "target_images = torch.stack(target_images)\n",
    "output_images_all = torch.stack(output_images_all)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Save datasets to disk\n",
    "torch.save({\n",
    "    'original_images': original_images,\n",
    "    'target_images': target_images,\n",
    "    'output_images_all': output_images_all,\n",
    "    'labels': labels\n",
    "}, os.path.join(save_dir, 'dataset.pt'))\n",
    "\n",
    "# Split dataset according to specifications\n",
    "train_original = original_images[:20000]\n",
    "train_target = target_images[:20000]\n",
    "train_output = output_images_all[:20000]\n",
    "train_labels = labels[:20000]\n",
    "\n",
    "buffer_original = original_images[20000:22500]\n",
    "buffer_target = target_images[20000:22500]\n",
    "buffer_output = output_images_all[20000:22500]\n",
    "buffer_labels = labels[20000:22500]\n",
    "\n",
    "test_original = original_images[22500:23750]\n",
    "test_target = target_images[22500:23750]\n",
    "test_output = output_images_all[22500:23750]\n",
    "test_labels = labels[22500:23750]\n",
    "\n",
    "val_original = original_images[23750:25000]\n",
    "val_target = target_images[23750:25000]\n",
    "val_output = output_images_all[23750:25000]\n",
    "val_labels = labels[23750:25000]\n",
    "\n",
    "# Save splits to disk\n",
    "torch.save({\n",
    "    'train_original': train_original,\n",
    "    'train_target': train_target,\n",
    "    'train_output': train_output,\n",
    "    'train_labels': train_labels,\n",
    "    'buffer_original': buffer_original,\n",
    "    'buffer_target': buffer_target,\n",
    "    'buffer_output': buffer_output,\n",
    "    'buffer_labels': buffer_labels,\n",
    "    'test_original': test_original,\n",
    "    'test_target': test_target,\n",
    "    'test_output': test_output,\n",
    "    'test_labels': test_labels,\n",
    "    'val_original': val_original,\n",
    "    'val_target': val_target,\n",
    "    'val_output': val_output,\n",
    "    'val_labels': val_labels\n",
    "}, os.path.join(save_dir, 'dataset_splits.pt'))\n",
    "\n",
    "# Create DataLoader objects\n",
    "batch_size = 32\n",
    "train_dataset = ImageDataset(train_original, train_target, train_output, train_labels, transform=transforms.ToTensor())\n",
    "test_dataset = ImageDataset(test_original, test_target, test_output, test_labels, transform=transforms.ToTensor())\n",
    "val_dataset = ImageDataset(val_original, val_target, val_output, val_labels, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 64 * 64, 128)  # Adjust according to the input image size\n",
    "        self.fc2 = nn.Linear(128, 4)  # 4 classes for 4 models\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 64 * 64)  # Adjust according to the input image size\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = CNNClassifier()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f'Accuracy of the model on the test images: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Auto-encoder training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the AutoEncoder model\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2)\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 3, kernel_size=2, stride=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the autoencoder model\n",
    "autoencoder = AutoEncoder()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class for the autoencoder\n",
    "class AutoEncoderDataset(Dataset):\n",
    "    def __init__(self, images, transform=None):\n",
    "        self.images = images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, image\n",
    "\n",
    "# Create DataLoader objects for the autoencoder training\n",
    "transform = transforms.ToTensor()\n",
    "autoencoder_dataset = AutoEncoderDataset(output_images_model1 + output_images_model2 + output_images_model3 + output_images_model4, transform=transform)\n",
    "autoencoder_loader = DataLoader(autoencoder_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the autoencoder\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    autoencoder.train()\n",
    "    running_loss = 0.0\n",
    "    for images, _ in autoencoder_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = autoencoder(images)\n",
    "        loss = criterion(outputs, images)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(autoencoder_loader):.4f}')\n",
    "\n",
    "# Save the trained autoencoder model\n",
    "torch.save(autoencoder.state_dict(), 'autoencoder.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Combining latent vectors and reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Load the trained classifier model\n",
    "classifier = CNNClassifier()\n",
    "classifier.load_state_dict(torch.load('classifier.pth'))\n",
    "classifier.eval()\n",
    "\n",
    "# Load the trained autoencoder model\n",
    "autoencoder = AutoEncoder()\n",
    "autoencoder.load_state_dict(torch.load('autoencoder.pth'))\n",
    "autoencoder.eval()\n",
    "\n",
    "# Define the encoder and decoder separately for easier handling\n",
    "encoder = autoencoder.encoder\n",
    "decoder = autoencoder.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to combine latent vectors with weights\n",
    "def combine_latent_vectors(vectors, weights):\n",
    "    combined_vector = sum(w * v for w, v in zip(weights, vectors))\n",
    "    return combined_vector\n",
    "\n",
    "# Assume you have an input image to process\n",
    "input_image_path = '/path/to/input/image.png'\n",
    "input_image = Image.open(input_image_path).resize((256, 256))\n",
    "input_image = transforms.ToTensor()(input_image).unsqueeze(0)  # Convert to tensor and add batch dimension\n",
    "\n",
    "# Get the output images from the four individual models\n",
    "output1 = model1(input_image)\n",
    "output2 = model2(input_image)\n",
    "output3 = model3(input_image)\n",
    "output4 = model4(input_image)\n",
    "\n",
    "# Pass these output images through the encoder part of the auto-encoder to get four latent vectors\n",
    "latent_vector1 = encoder(output1).detach()\n",
    "latent_vector2 = encoder(output2).detach()\n",
    "latent_vector3 = encoder(output3).detach()\n",
    "latent_vector4 = encoder(output4).detach()\n",
    "\n",
    "# Use the classifier to get the softmax weights for the four latent vectors\n",
    "with torch.no_grad():\n",
    "    weights = classifier(input_image).softmax(dim=1).squeeze()\n",
    "\n",
    "# Combine these latent vectors using the weights to get a single combined latent vector\n",
    "combined_vector = combine_latent_vectors([latent_vector1, latent_vector2, latent_vector3, latent_vector4], weights)\n",
    "\n",
    "# Pass the combined latent vector through the decoder part of the auto-encoder to get the final output image\n",
    "final_output_image = decoder(combined_vector.unsqueeze(0))\n",
    "\n",
    "# Convert the output tensor to an image format and save it\n",
    "final_output_image = final_output_image.squeeze().permute(1, 2, 0).numpy()\n",
    "final_output_image = (final_output_image * 255).astype(np.uint8)\n",
    "save_path = '/path/to/output/image.png'\n",
    "Image.fromarray(final_output_image).save(save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
