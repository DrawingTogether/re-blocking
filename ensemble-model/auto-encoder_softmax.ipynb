{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Data generation and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, input_images, output_images, labels, transform=None):\n",
    "        self.input_images = input_images\n",
    "        self.output_images = output_images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.output_images[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Function to load images from a directory\n",
    "def load_images_from_directory(directory, target_size=(256, 256)):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(target_size),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    images = []\n",
    "    for filename in sorted(os.listdir(directory)):\n",
    "        img_path = os.path.join(directory, filename)\n",
    "        img = datasets.folder.default_loader(img_path)\n",
    "        img = transform(img)\n",
    "        images.append(img)\n",
    "    return torch.stack(images)\n",
    "\n",
    "# Directories\n",
    "input_dir = '/path/to/dataset/input_images'\n",
    "output_dir1 = '/path/to/dataset/output_model1'\n",
    "output_dir2 = '/path/to/dataset/output_model2'\n",
    "output_dir3 = '/path/to/dataset/output_model3'\n",
    "output_dir4 = '/path/to/dataset/output_model4'\n",
    "\n",
    "# Load images\n",
    "input_images = load_images_from_directory(input_dir)\n",
    "output_images_model1 = load_images_from_directory(output_dir1)\n",
    "output_images_model2 = load_images_from_directory(output_dir2)\n",
    "output_images_model3 = load_images_from_directory(output_dir3)\n",
    "output_images_model4 = load_images_from_directory(output_dir4)\n",
    "\n",
    "# Create dataset and labels\n",
    "dataset = []\n",
    "labels = []\n",
    "for i in range(len(input_images)):\n",
    "    dataset.append(output_images_model1[i])\n",
    "    labels.append(0)  # Label for model 1\n",
    "    dataset.append(output_images_model2[i])\n",
    "    labels.append(1)  # Label for model 2\n",
    "    dataset.append(output_images_model3[i])\n",
    "    labels.append(2)  # Label for model 3\n",
    "    dataset.append(output_images_model4[i])\n",
    "    labels.append(3)  # Label for model 4\n",
    "\n",
    "# Convert dataset and labels to tensors\n",
    "dataset = torch.stack(dataset)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create DataLoader objects\n",
    "batch_size = 32\n",
    "train_dataset = ImageDataset(input_images, X_train, y_train, transform=transforms.ToTensor())\n",
    "test_dataset = ImageDataset(input_images, X_test, y_test, transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 64 * 64, 128)  # Adjust according to the input image size\n",
    "        self.fc2 = nn.Linear(128, 4)  # 4 classes for 4 models\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 64 * 64)  # Adjust according to the input image size\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = CNNClassifier()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f'Accuracy of the model on the test images: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Auto-encoder training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the AutoEncoder model\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2)\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 3, kernel_size=2, stride=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the autoencoder model\n",
    "autoencoder = AutoEncoder()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class for the autoencoder\n",
    "class AutoEncoderDataset(Dataset):\n",
    "    def __init__(self, images, transform=None):\n",
    "        self.images = images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, image\n",
    "\n",
    "# Create DataLoader objects for the autoencoder training\n",
    "transform = transforms.ToTensor()\n",
    "autoencoder_dataset = AutoEncoderDataset(output_images_model1 + output_images_model2 + output_images_model3 + output_images_model4, transform=transform)\n",
    "autoencoder_loader = DataLoader(autoencoder_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the autoencoder\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    autoencoder.train()\n",
    "    running_loss = 0.0\n",
    "    for images, _ in autoencoder_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = autoencoder(images)\n",
    "        loss = criterion(outputs, images)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(autoencoder_loader):.4f}')\n",
    "\n",
    "# Save the trained autoencoder model\n",
    "torch.save(autoencoder.state_dict(), 'autoencoder.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Combining latent vectors and reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Load the trained classifier model\n",
    "classifier = CNNClassifier()\n",
    "classifier.load_state_dict(torch.load('classifier.pth'))\n",
    "classifier.eval()\n",
    "\n",
    "# Load the trained autoencoder model\n",
    "autoencoder = AutoEncoder()\n",
    "autoencoder.load_state_dict(torch.load('autoencoder.pth'))\n",
    "autoencoder.eval()\n",
    "\n",
    "# Define the encoder and decoder separately for easier handling\n",
    "encoder = autoencoder.encoder\n",
    "decoder = autoencoder.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to combine latent vectors with weights\n",
    "def combine_latent_vectors(vectors, weights):\n",
    "    combined_vector = sum(w * v for w, v in zip(weights, vectors))\n",
    "    return combined_vector\n",
    "\n",
    "# Assume you have an input image to process\n",
    "input_image_path = '/path/to/input/image.png'\n",
    "input_image = Image.open(input_image_path).resize((256, 256))\n",
    "input_image = transforms.ToTensor()(input_image).unsqueeze(0)  # Convert to tensor and add batch dimension\n",
    "\n",
    "# Get the output images from the four individual models\n",
    "output1 = model1(input_image)\n",
    "output2 = model2(input_image)\n",
    "output3 = model3(input_image)\n",
    "output4 = model4(input_image)\n",
    "\n",
    "# Pass these output images through the encoder part of the auto-encoder to get four latent vectors\n",
    "latent_vector1 = encoder(output1).detach()\n",
    "latent_vector2 = encoder(output2).detach()\n",
    "latent_vector3 = encoder(output3).detach()\n",
    "latent_vector4 = encoder(output4).detach()\n",
    "\n",
    "# Use the classifier to get the softmax weights for the four latent vectors\n",
    "with torch.no_grad():\n",
    "    weights = classifier(input_image).softmax(dim=1).squeeze()\n",
    "\n",
    "# Combine these latent vectors using the weights to get a single combined latent vector\n",
    "combined_vector = combine_latent_vectors([latent_vector1, latent_vector2, latent_vector3, latent_vector4], weights)\n",
    "\n",
    "# Pass the combined latent vector through the decoder part of the auto-encoder to get the final output image\n",
    "final_output_image = decoder(combined_vector.unsqueeze(0))\n",
    "\n",
    "# Convert the output tensor to an image format and save it\n",
    "final_output_image = final_output_image.squeeze().permute(1, 2, 0).numpy()\n",
    "final_output_image = (final_output_image * 255).astype(np.uint8)\n",
    "save_path = '/path/to/output/image.png'\n",
    "Image.fromarray(final_output_image).save(save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
